## Overview

![flowchar-img](assets/flowchart_r.png)
Our framework consists of two main stages:

- Stage A: Sparse-Coded Masked Brain Modeling (_SC-MBM_)
- Stage B: Double-Conditioned Latent Diffusion Model (_DC-LDM_)

The **data** folder **pretrains** folder are not included in this repository.
Please download them from [Gdrive](https://drive.google.com/drive/folders/1cetXxe2Syi817Ur1yhc09MQtDioiV3az?usp=sharing) and put them in the root directory of this repository as shown below.

File path | Description

```

/data
â”£ ðŸ“‚ NOD
â”ƒ   â”£ ðŸ“‚ ciftify
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub-01
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub-02
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”£ img_dict.npy

/pretrains
â”£ ðŸ“‚ ldm
â”ƒ   â”£ ðŸ“‚ label2img  ï¼ˆImageNet pre-trained label-conditioned LDM)
â”ƒ   â”ƒ   â”— ðŸ“œ config.yaml
â”ƒ   â”ƒ   â”— ðŸ“œ model.ckpt

/results
â”£ ðŸ“‚ eval
â”£ ðŸ“‚ fmri_finetune
â”£ ðŸ“‚ fmri_pretrain
â”£ ðŸ“‚ generation


/code
â”£ ðŸ“‚ sc_mbm
â”ƒ   â”— ðŸ“œ mae_for_fmri.py
â”ƒ   â”— ðŸ“œ trainer.py
â”ƒ   â”— ðŸ“œ utils.py

â”£ ðŸ“‚ dc_ldm
â”ƒ   â”— ðŸ“œ ldm_for_fmri.py
â”ƒ   â”— ðŸ“œ utils.py
â”ƒ   â”£ ðŸ“‚ models
â”ƒ   â”ƒ   â”— (adopted from LDM)
â”ƒ   â”£ ðŸ“‚ modules
â”ƒ   â”ƒ   â”— (adopted from LDM)

â”—  ðŸ“œ stageA1_mbm_pretrain.py   (main script for pre-training for SC-MBM)
â”—  ðŸ“œ stageA2_mbm_finetune.py   (main script for tuning SC-MBM on fMRI only from test sets)
â”—  ðŸ“œ stageB_ldm_finetune.py    (main script for fine-tuning DC-LDM)
â”—  ðŸ“œ gen_eval.py               (main script for generating decoded images)

â”—  ðŸ“œ dataset.py                (functions for loading datasets)
â”—  ðŸ“œ eval_metrics.py           (functions for evaluation metrics)
â”—  ðŸ“œ config.py                 (configurations for the main scripts)

```

## Environment setup

Create and activate conda environment named `mind-vis` from our `env.yaml`

```sh
conda env create -f env.yaml
conda activate mind-vis
```

## Download data and checkpoints

We have provided the data and checkpoints for one subject(sub-08) to directly run the evaluation script that is described in further below.

We also provide checkpoints to skip all the training/finetuning and directly run the final evaluation script(described below) at [FigShare](limk). Due to the size limit, we only release the checkpoints for Subject 8 NOD. After downloading, extract the `results` folder to the project directory.

## SC-MBM Pre-training on fMRI (Stage A)

We are using the checkpoint provided by the MinD-Vis authors for SC-MBM pretrain(present in `results/fmri_pretrain`). To finetune the encoder execute the following batch file on HPC.

```sh
sbatch run-stagea2.SBATCH
```

The fMRI finetuning results will be saved locally at `results/fmri_finetune` and remotely at `wandb`.

## Finetune the Double-Conditional LDM with Pre-trained fMRI Encoder (Stage B)

In this stage, the cross-attention heads and pre-trained fMRI encoder will be jointly optimized with fMRI-image pairs. Decoded images will be generated in this stage.

```sh
sbatch run-stageb.SBATCH
```

Inside this sbatch file set the `--pretrain_mbm_path` option to the checkpoint generated by the previous stage. The results and generated samples will be saved locally at `results/generation` and remotely at ``wandb`.

## Run fMRI Decoding and Generate Images with Trained Checkpoints

You can directly run this stage with our provided checkpoints from the `results` folder:

```sh
sbatch run-eval.SBATCH
```

The results and generated samples will be saved locally at `results/eval` and remotely at `wandb`.

## Acknowledgement

We thank [NOD team](https://github.com/GongZhengxin/NOD-fmri) for making their raw and pre-processed data public. We also extend our gratitude toward the [MinD-Vis team](https://github.com/zjc062/mind-vis/tree/main) for making their codes and checkpoints publicly available!
