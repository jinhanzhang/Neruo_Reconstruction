## Overview

![flowchar-img](assets/flowchart_r.png)
Our framework consists of two main stages:

- Stage A: Sparse-Coded Masked Brain Modeling (_SC-MBM_)
- Stage B: Double-Conditioned Latent Diffusion Model (_DC-LDM_)

The **data** folder **pretrains** folder are not included in this repository.
Please download them from [FigShare](https://figshare.com/s/94cd778e6afafb00946e) and put them in the root directory of this repository as shown below.

File path | Description

```

/data
â”£ ðŸ“‚ NOD
â”ƒ   â”£ ðŸ“‚ ciftify
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub-01
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub-02
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”£ img_dict.npy

/pretrains
â”£ ðŸ“‚ ldm
â”ƒ   â”£ ðŸ“‚ label2img  ï¼ˆImageNet pre-trained label-conditioned LDM)
â”ƒ   â”ƒ   â”— ðŸ“œ config.yaml
â”ƒ   â”ƒ   â”— ðŸ“œ model.ckpt

/results
â”£ ðŸ“‚ eval
â”£ ðŸ“‚ fmri_finetune
â”£ ðŸ“‚ fmri_pretrain
â”£ ðŸ“‚ generation


/code
â”£ ðŸ“‚ sc_mbm
â”ƒ   â”— ðŸ“œ mae_for_fmri.py
â”ƒ   â”— ðŸ“œ trainer.py
â”ƒ   â”— ðŸ“œ utils.py

â”£ ðŸ“‚ dc_ldm
â”ƒ   â”— ðŸ“œ ldm_for_fmri.py
â”ƒ   â”— ðŸ“œ utils.py
â”ƒ   â”£ ðŸ“‚ models
â”ƒ   â”ƒ   â”— (adopted from LDM)
â”ƒ   â”£ ðŸ“‚ modules
â”ƒ   â”ƒ   â”— (adopted from LDM)

â”—  ðŸ“œ stageA1_mbm_pretrain.py   (main script for pre-training for SC-MBM)
â”—  ðŸ“œ stageA2_mbm_finetune.py   (main script for tuning SC-MBM on fMRI only from test sets)
â”—  ðŸ“œ stageB_ldm_finetune.py    (main script for fine-tuning DC-LDM)
â”—  ðŸ“œ gen_eval.py               (main script for generating decoded images)

â”—  ðŸ“œ dataset.py                (functions for loading datasets)
â”—  ðŸ“œ eval_metrics.py           (functions for evaluation metrics)
â”—  ðŸ“œ config.py                 (configurations for the main scripts)

```

## Environment setup

Create and activate conda environment named `mind-vis` from our `env.yaml`

```sh
conda env create -f env.yaml
conda activate mind-vis
```

## Download data and checkpoints

We have provided the data and checkpoints for one subject(sub-08) to directly run the evaluation script that is described in further below.

We also provide checkpoints and finetuning data at [FigShare](https://figshare.com/s/94cd778e6afafb00946e) to run the finetuing and decoding directly. Due to the size limit, we only release the checkpoints for Subject 3 and CSI1 in the GOD and BOLD5000 respectively. Checkpoints for other subjects are also available upon request. After downloading, extract the `data/` and `pretrains/` to the project directory.

## SC-MBM Pre-training on fMRI (Stage A)

We are using the checkpoint provided by the MinD-Vis authors for SC-MBM pretrain. To finetune the encoder execute the following batch file on HPC.

```sh
sbatch run-stagea2.SBATCH
```

The fMRI finetuning results will be saved locally at `results/fmri_finetune` and remotely at `wandb`.

## Finetune the Double-Conditional LDM with Pre-trained fMRI Encoder (Stage B)

In this stage, the cross-attention heads and pre-trained fMRI encoder will be jointly optimized with fMRI-image pairs. Decoded images will be generated in this stage.

```sh
sbatch run-stageb.SBATCH
```

Inside this sbatch file set the `--pretrain_mbm_path` option to the checkpoint generated by the previous stage. The results and generated samples will be saved locally at `results/generation` and remotely at ``wandb`.

## Run fMRI Decoding and Generate Images with Trained Checkpoints

Only finetuning datasets and trained checkpoints in our FigShare link are required. Notice that images generated by the provided checkpoins gives the same evaluation reuslts as in the paper, but may not produce the exact same images as in the paper due to sampling variance. Run this stage with our provided checkpoints:

```sh
python code/gen_eval.py --dataset GOD
```

`--dataset` can be either `GOD` or `BOLD5000`. The results and generated samples will be saved locally at `results/eval` and remotely at `wandb`.

![bold5000](assets/bold5000.png)

## Acknowledgement

We thank [NOD team](https://github.com/GongZhengxin/NOD-fmri) for making their raw and pre-processed data public. We also extend our gratitude toward the [MinD-Vis team](https://github.com/zjc062/mind-vis/tree/main) for making their codes and checkpoints publicly available!
